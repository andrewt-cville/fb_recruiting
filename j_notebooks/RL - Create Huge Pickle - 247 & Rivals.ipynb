{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecordLinkage Linking - 247 & Rivals\n",
    "\n",
    "> Leveraging the RL library to determine approximate matching over a range of fields using various string methods methods.  This specifically focuses on 247 & Rivals first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import time\n",
    "import os\n",
    "import recordlinkage\n",
    "import csv\n",
    "import core_constants as cc\n",
    "import functions as fx\n",
    "\n",
    "#not currently using jellyfish\n",
    "import jellyfish as jf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I don't like having all of this here - should push to functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDir = '..//scrapedData//'\n",
    "field_agg = \"_\"\n",
    "\n",
    "## Load the source file dict\n",
    "sourceFiles = json.loads(open('..//config//sourceFiles.json', \"r\").read())\n",
    "\n",
    "## Load the id config\n",
    "idConfig = json.loads(open('..//config//idConfig.json', \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Merge Source Files Then Create a List of Dicts for each Dataset\n",
    "> This was originally set up for all of the keys in the sourcefiles.json config.  Since this file is currently only going to serve 247 & Rivals, I've hardcoded the keys to fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_keys = ['sports247', 'rivals']\n",
    "dfs = []\n",
    "for key in dataset_keys:\n",
    "    vars()[key] = fx.mergeSourceFiles (key, outputDir, sourceFiles)    \n",
    "    dfs.append(vars()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New IDs\n",
    "> This isn't elegant and I'd love to basically preprocess these collections prior to this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx.createNewID(idConfig['sports247'], sports247, field_agg)\n",
    "fx.createNewID(idConfig['rivals'], rivals, field_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes\n",
    "> I might move these to their own pickles so I don't constantly have to recreate these each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sports247 = pandas.DataFrame(sports247)\n",
    "df_rivals = pandas.DataFrame(rivals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename the indexes to something readable and predictable\n",
    "> Otherwise these become 'index' and 'level_0', etc when you start to merge these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sports247.index.name = '247_index'\n",
    "df_rivals.index.name = 'rivals_index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Blockers\n",
    "> I swear this isn't working.  And honestly I guess I'm ok with it not working since I'm doing string operations later on this script.  This usually takes 20+ minutes - so it is doing something..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = recordlinkage.Index()\n",
    "indexer.block('school')\n",
    "indexer.block('year')\n",
    "candidate_links = indexer.index(df_sports247, df_rivals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Features\n",
    "\n",
    "> These are all pretty straightforward.  The toughest one to assess is position - since the services don't always categorize players in the same way or have the same abbreviation for a single position.  Since this is often only a 2 or 3 letter string, I decided to do an exact match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compare>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = recordlinkage.Compare()\n",
    "\n",
    "c.exact('ID', 'ID', label='ID')\n",
    "c.string('playerName', 'playerName', method='damerau_levenshtein', label='playerName')\n",
    "c.string('school', 'school', label='college')\n",
    "c.string('city', 'city', label='city')\n",
    "c.exact('state', 'state', label='state')\n",
    "c.string('highSchool', 'highSchool', label='highSchool')\n",
    "c.exact('position', 'position', label='position')\n",
    "c.exact('year', 'year', label='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comparison Vector Set and Output to Pickle\n",
    "> This will take 60+ minutes to complete.  The output will be an 8+ GB pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = c.compute(candidate_links, df_sports247, df_rivals)\n",
    "features.to_pickle(\"features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
