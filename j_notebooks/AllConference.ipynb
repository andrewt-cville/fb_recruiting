{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: All Conference\n",
    "#### Sources: Wikipedia, Manual Data Aggregation (CSV)\n",
    "> The All Conference dataset works a bit differently.  Half of the data comes from Wikipedia, but Wiki doesn't have standard representations of all the conferences.  Instead, work was done to manually grab that data from multiple disparate sources and normalized into a single CSV file.  This notebook will step through getting data from both sources, normalizing and integrating the findings before summarizing for the final linking step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import core_constants as cc\n",
    "import functions as fx\n",
    "import requests\n",
    "import lxml\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "import recordlinkage\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the page titles you are interested in\n",
    "pageList = [['sec', 'All-SEC_football_team'], ['bigtwelve', 'All-Big_12_Conference_football_team'], ['bigten', 'All-Big_Ten_Conference_football_team']]\n",
    "years = cc.get_defYears()\n",
    "headers= cc.get_header()\n",
    "csvFile = \"..//scrapedData//allConf.csv\"\n",
    "dataset = 'AllConference'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get & Save the Wikipedia HTML\n",
    "#### Source: https://en.wikipedia.org/wiki/2020_Big_Ten_Conference_football_season#All-Conference_Teams\n",
    "> These Wiki pages contain the all conference records.  We cycle through by conference - which for these three conferences follow the same page layout and url schema for each year we care about.  \n",
    "\n",
    "##### To run - convert the below cell back to code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx.get_WikipediaAllConf(pageList, headers, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes B1G, SEC and Big 12 on Wikipedia.\n",
    "> Error outputs are caught exceptions.  The JSON file will still write out but you can go back to wikipedia to figure out why it failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - Couldn\"t write record: , Ole Miss (ESPN) Year:2015\n"
     ]
    }
   ],
   "source": [
    "teamDir = ['..//html//wikipedia//allconference//bigten//', '..//html//wikipedia//allconference//bigtwelve//', '..//html//wikipedia//allconference//sec//']\n",
    "\n",
    "with open(\"..//scrapedData//Wiki_AllConf.json\", \"w\") as write_file:\n",
    "    json.dump(fx.process_wikiConferences(teamDir), write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get & Process the CSV File for the other Conferences\n",
    "\n",
    "> The CSV file contains the all conference records for the Pac12, ACC and the group of 5 conferences\n",
    "\n",
    "##### Input: //scrapedData/allconf.csv\n",
    "##### Output: //scrapedData/allConf.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..//scrapedData//allConf.json\", \"w\") as write_file:\n",
    "    write_file.write(json.dumps(fx.process_csvAllConf(fx.get_csvAllConf(csvFile))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear DB\n",
    ">Useful for a clean start.  THis removes all of the records for this dataset from the following structures: SourcedPlayers, REcordLinks.  All of the Views auto-cleanse themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx.clearDB(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DB Write is done'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.toDB_AllConference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strict Matching\n",
    "> This saves to RecordLinking where ID == ID, but returns IDYR as the matching targetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SQLite\n"
     ]
    }
   ],
   "source": [
    "fx.literalLinking(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching w/ Threshold\n",
    "> This is automatically pushing fuzzy matches above a certain threshold into the DB without the need for review.  \n",
    "\n",
    ">LIES - currently all this does is create a dataframe that is used to create the annotation file.  The thresholds are terribly hardcoded in the source function.  Needs to be cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzyDF = fx.doFuzzyMatching(dataset, 'Sports247')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Annotation File\n",
    ">This changes the dataframe into a MultiIndex data frame that the annotation function requires.\n",
    "\n",
    ">Don't forget that the length needs to change below in line 25, based on how large the fuzzyDF is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sql.connect(cc.databaseName) \n",
    "          \n",
    "sql_query = pd.read_sql_query ('''\n",
    "                               SELECT\n",
    "                               *\n",
    "                               FROM SourcedPlayers\n",
    "                               WHERE KeyDataSet = 1\n",
    "                               ''', conn)\n",
    "\n",
    "df_247 = pd.DataFrame(sql_query, columns = ['IDYR', 'College', 'Year', 'Position'])\n",
    "df_247.set_index('IDYR', append=False, inplace=True)\n",
    "sql_query = pd.read_sql_query ('''\n",
    "                               SELECT\n",
    "                               *\n",
    "                               FROM UnlinkedAllConference\n",
    "                               ''', conn)\n",
    "\n",
    "df_AllConference = pd.DataFrame(sql_query, columns = ['ID', 'College'])\n",
    "df_AllConference.set_index('ID', append=False, inplace=True)\n",
    "\n",
    "fuzzyMI = pd.MultiIndex.from_frame(fuzzyDF)\n",
    "recordlinkage.write_annotation_file(\n",
    "    \"../Annotations/Annotations/annotation_allconference.json\",\n",
    "    fuzzyMI[0:300],\n",
    "    df_NCAA,\n",
    "    df_247,\n",
    "    dataset_a_name=\"AllConference\",\n",
    "    dataset_b_name=\"Master\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Annotation File\n",
    "> After manually annotating using the RecordLinkage annotator tool, this takes the resulting Annotation file and places it into a dict for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = recordlinkage.read_annotation_file(\"..//Annotations//Results//allconf_results.json\")\n",
    "try:\n",
    "    annotation_dict = (annotation.links).to_flat_index()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Annotation dict into RecordLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in annotation_dict:\n",
    "    #MAKE SURE YOU UPDATE THE THIRD VALUE TO THE CORRECT KEYDATASET!!\n",
    "    Values = [record[0], record[1], 4, 1, 1]\n",
    "    query = '''INSERT INTO RecordLinks(MasterID, TargetID, KeyDataSet, KeyLinkType, LinkConfidence)\n",
    "        VALUES (?,?,?,?,?)'''\n",
    "    \n",
    "    conn = sql.connect(cc.databaseName)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    c.execute(query, Values)\n",
    "    conn.commit()\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
