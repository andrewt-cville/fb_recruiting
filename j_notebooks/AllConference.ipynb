{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: All Conference\n",
    "#### Sources: Wikipedia, Manual Data Aggregation (CSV)\n",
    "> The All Conference dataset works a bit differently.  Half of the data comes from Wikipedia, but Wiki doesn't have standard representations of all the conferences.  Instead, work was done to manually grab that data from multiple disparate sources and normalized into a single CSV file.  This notebook will step through getting data from both sources, normalizing and integrating the findings before summarizing for the final linking step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import core_constants as cc\n",
    "import functions as fx\n",
    "import requests\n",
    "import lxml\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import sqlite3 as sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the page titles you are interested in\n",
    "pageList = [['sec', 'All-SEC_football_team'], ['bigtwelve', 'All-Big_12_Conference_football_team'], ['bigten', 'All-Big_Ten_Conference_football_team']]\n",
    "years = cc.get_defYears()\n",
    "headers= cc.get_header()\n",
    "csvFile = \"..//scrapedData//allConf.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get & Save the Wikipedia HTML\n",
    "#### Source: https://en.wikipedia.org/wiki/2020_Big_Ten_Conference_football_season#All-Conference_Teams\n",
    "> These Wiki pages contain the all conference records.  We cycle through by conference - which for these three conferences follow the same page layout and url schema for each year we care about.  \n",
    "\n",
    "##### To run - convert the below cell back to code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fx.get_WikipediaAllConf(pageList, headers, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process & Save Big Ten/Big 12 Data from Wikipedia\n",
    "> These files are already saved locally.  Saves it out to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamDir = ['..//html//wikipedia//allconference//bigten//', '..//html//wikipedia//allconference//bigtwelve//']\n",
    "\n",
    "with open(\"..//scrapedData//Ten12AllConf.json\", \"w\") as write_file:\n",
    "    json.dump(fx.process_WikipediaBigTenBigTwelve(teamDir), write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process & Save SEC Data from Wikipedia\n",
    "> These files are already saved locally.  Saves it out to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamDir = '..//html//wikipedia//allconference//sec//'\n",
    "\n",
    "with open(\"..//scrapedData//SECAllConf.json\", \"w\") as write_file:\n",
    "    json.dump(fx.process_WikipediaSEC(teamDir), write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get & Process the CSV File for the other Conferences\n",
    "\n",
    "> The CSV file contains the all conference records for the Pac12, ACC and the group of 5 conferences\n",
    "\n",
    "##### Input: //scrapedData/allconf.csv\n",
    "##### Output: //scrapedData/allConf.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..//scrapedData//allConf.json\", \"w\") as write_file:\n",
    "    write_file.write(json.dumps(fx.process_csvAllConf(fx.get_csvAllConf(csvFile))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the Dataset\n",
    "\n",
    "> We don't need repetitive fields across the various datasets (ie - I don't need height coming back from 3 sources).  This step strips to only what I care about for the master print out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDir = '..//summarizedData//'\n",
    "dataset = 'allConf'\n",
    "\n",
    "with open(outputDir + dataset + \".json\", \"w\", encoding=\"utf-8\") as write_file:\n",
    "                write_file.write(json.dumps(fx.summarize_allConf()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DB Write is done'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.toDB_AllConference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
