{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Files for Manual Linking\n",
    ">This notebook facilitates writing out non-linked players to an annotation file - then handling the write out to annotation files.  The files can then be loaded here to process: https://j535d165.github.io/recordlinkage-annotator/  Finally the result file can be loaded back into the j_notebook directory where it is picked up here and added to the Record Links database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.index import Block\n",
    "import sqlite3 as sql\n",
    "import functions as fx\n",
    "import core_constants as cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "> Load the source and target datasets.  Currently it is only written for 247 and Rivals but needs to be extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keydatasets = [1, 2]\n",
    "dataset_names = []\n",
    "for ds in keydatasets:\n",
    "    SQL = ''' SELECT * FROM DataSet'''\n",
    "    datasets = (fx.connDBAndReturnDF(SQL)).to_dict('records')\n",
    "    dataset = next(item for item in datasets if item[\"KeyDataSet\"] == ds)\n",
    "    SQL = \"\"\"SELECT * FROM {}\"\"\".format(dataset['UnlinkedView'])\n",
    "    \n",
    "    vars()[dataset['DataSet']] = (fx.connDBAndReturnDF(SQL)).set_index('IDYR')\n",
    "    (vars()[dataset['DataSet']]).index.name = dataset['DataSet'] + '_IDYR'\n",
    "    (vars()[dataset['DataSet']])['IDYR'] = (vars()[dataset['DataSet']]).index.get_level_values(0)\n",
    "    dataset_names.append(vars() [dataset['DataSet']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = ''' SELECT TargetFieldName FROM Features where KeyMasterDataSet = {} and KeyTargetDataSet = {} and MatchType = \"block\"'''.format(keydatasets[0], keydatasets[1])\n",
    "blockersRaw = (fx.connDBAndReturnDF(SQL)).to_dict('records')\n",
    "blockers = []\n",
    "for i in blockersRaw:\n",
    "    for key,value in i.items():\n",
    "        blockers.append(value)\n",
    "\n",
    "indexer = rl.BlockIndex(on = blockers)\n",
    "candidate_links = indexer.index(dataset_names[0], dataset_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = rl.Compare()\n",
    "\n",
    "SQL = '''SELECT TargetFieldName, MatchType, Method FROM Features where KeyMasterDataSet = {} and KeyTargetDataSet = {} and MatchType <> \"block\"'''.format(keydatasets[0], keydatasets[1])\n",
    "featureFields = (fx.connDBAndReturnDF(SQL).to_dict('records'))\n",
    "\n",
    "for feature in featureFields:\n",
    "    \n",
    "    if (feature['MatchType'] == 'exact'):\n",
    "        c.exact(feature['TargetFieldName'], feature['TargetFieldName'], label = feature['TargetFieldName'])\n",
    "    elif(feature['MatchType'] == 'string' and feature['Method'] is None):\n",
    "        c.string(feature['TargetFieldName'], feature['TargetFieldName'], label = feature['TargetFieldName'])\n",
    "    elif(feature['MatchType'] == 'string' and feature['Method'] is not None):\n",
    "        c.string(feature['TargetFieldName'], feature['TargetFieldName'], method = feature['Method'], label = feature['TargetFieldName'])\n",
    "\n",
    "try:\n",
    "    features = c.compute(candidate_links, dataset_names[0], dataset_names[1])\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for column in features:\n",
    "    sum = sum + features[column]\n",
    "\n",
    "print(len(featureFields))\n",
    "features['Sum'] = sum/len(featureFields)\n",
    "\n",
    "features.to_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Blockers\n",
    "> In this case, we are creating blockers on college and year to limit the number of possible matches.\n",
    "\n",
    "#### !!!This should be extended to take into account fuzzy matching confidence level as well!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = rl.BlockIndex(on=['College', 'Year'])\n",
    "candidate_links = indexer.index(sports247, rivals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Annotation file\n",
    "> This is actually terrible code.  Currently I'm manually updating by length.  Need to loop through.  TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.write_annotation_file(\n",
    "    \"..//Annotations//Annotations//annotate_rivals_14.json\",\n",
    "    candidate_links[7001:7500],\n",
    "    sports247,\n",
    "    rivals,\n",
    "    dataset_a_name=\"247 Sports\",\n",
    "    dataset_b_name=\"Rivals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Result\n",
    "> Once you handled the above file in the annotator, the below code will verify that the result file is correctly located in the j_notebooks folder and will read it into a flat index - which makes it easier to insert into the db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = rl.read_annotation_file(\"..//Annotations//Results//result_4.json\")\n",
    "try:\n",
    "    annotation_dict = (annotation.links).to_flat_index()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert into the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in annotation_dict:\n",
    "    Values = [record[0], record[1], 2, 1, 1]\n",
    "    query = '''INSERT INTO RecordLinks(MasterID, TargetID, KeyDataSet, KeyLinkType, LinkConfidence)\n",
    "        VALUES (?,?,?,?,?)'''\n",
    "    \n",
    "    conn = sql.connect(cc.databaseName)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    c.execute(query, Values)\n",
    "    conn.commit()\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
